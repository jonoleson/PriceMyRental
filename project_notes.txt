6/22:
-Using sklearn's implementation of KDTree, as it can be pickled, unlike scipy's
-Study KDTrees to explain in presentation


6/21:
-Don't forget; geopy.geocoders.Nomanatim
-Working on implementing postgis for median neighbor price model
-Need to create main.py for running model off user input

6/20:
-median_neighbors started executing at 8:04pm, finished at: n/a
-median_neighbors will not work as written; need to rewrite, use postgres, figure out additional filtering options

6/19:
-geopandas not installing correctly
-'%autoreload 0' to disable autoreload, '%autoreload 2' to reenable
-Look into geopy Nomanatim for address > coords 


6/18:
-Need to add commenting to models code!!
-For tomorrow, remember geopy.vincenty (for distance calcs) and look up geopandas

6/16:
-TFIDF vectorization on descriptions > NMF factorization adds latent feature weights to each point > Regressor to find price:avg ratio > Multiplied by 
        
-How to get neighborhood price averages if I only have lat and lon?
    -Add features to each listing: Neighborhood averages for 1bd, 2bd, 3bd, etc...
    -Or just a neighborhood weight coefficient
    -Apply clustering to lat/lon data?
    	-Geographical dividers (freeways/infrastructure, severe topography could complicate geographical clustering) 
    	-Weighted linear clustering? 
    -Zillow API may have neighborhood boundary data
    -If starting with just San Francisco, nearly all data points have neighborhood attributes (craigslist sf is set up so that posters will almost always select a neighborhood)
    	-Zillow API may have neighborhood boundary data

-Note: Would be easy to add Oakland data if I want, it has clean neighborhood attributes

-Can use average prices from internal craigslist data, or from external source?
    -Which would be more accurate? Internal data may have some bias in what types of properties end up listed on craigslist.
    -But if it's intended for people posting on craigslist, the craigslist bias may be a good thing 
    -But if this is intended 


6/15: 
-Models to compare: ridge, lasso, random forest regressor

-perhaps group data with classification model (into pricing tiers?), do regressions within each group (classification + regression)
    -Simplest: Classify each unit as either below market average or above market average relative to area and number of bedrooms
    -Better: 3-star rating; 1-star, 2-star, and 3-star ratings for each listing, with a regression model fitted to each
        -Use MN Naive Bayes to classify star rating?

-create separate csv of new-found amenities from external sources or text analysis

-Fraud detection, look into craigslist fraud guide

-Get unit star-rating by detecting features in description through string-matching (or MN Naive Bayes?)
    -Words to match: 
        -hardwood
        -marble
        -remodel
        -shuttle
        -bart
        -deck/balcony
        -garden
        -natural light/bright/sunlight
        -luxury
        -valet
        -designer
        -stainless

-Subset data for a homogenous set (eg, 2 bedrooms in the castro), then analyze what textual features of the description are highly correlated with upticks or downticks in price